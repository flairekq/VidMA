{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1495,
     "status": "ok",
     "timestamp": 1679821491459,
     "user": {
      "displayName": "bel hello",
      "userId": "14145485794765337692"
     },
     "user_tz": -480
    },
    "id": "qMvZUdHqAJ99"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1783,
     "status": "ok",
     "timestamp": 1679821612546,
     "user": {
      "displayName": "bel hello",
      "userId": "14145485794765337692"
     },
     "user_tz": -480
    },
    "id": "2-g7y5LOl6K0"
   },
   "outputs": [],
   "source": [
    "current_directory = \"/data\"\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22824,
     "status": "ok",
     "timestamp": 1679821637287,
     "user": {
      "displayName": "bel hello",
      "userId": "14145485794765337692"
     },
     "user_tz": -480
    },
    "id": "hCuoWxrZVf0I",
    "outputId": "6e0f7551-4c22-4502-dd44-2e48b23e5a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ntlk (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for ntlk\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting num2words\n",
      "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=99bce473c0f841f10e6f70b05dd12b6f02d67ca929de59147018d5a6706d5ad6\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, num2words\n",
      "Successfully installed docopt-0.6.2 num2words-0.5.12\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.20.9\n",
      "  Downloading Levenshtein-0.20.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.20.9 python-Levenshtein-0.20.9 rapidfuzz-2.13.7\n"
     ]
    }
   ],
   "source": [
    "!pip install ntlk\n",
    "!pip install num2words\n",
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1194,
     "status": "ok",
     "timestamp": 1679821638477,
     "user": {
      "displayName": "bel hello",
      "userId": "14145485794765337692"
     },
     "user_tz": -480
    },
    "id": "DJxa8D2k_qsg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag, ne_chunk\n",
    "from nltk import RegexpParser\n",
    "from nltk import Tree\n",
    "\n",
    "import re\n",
    "import num2words\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1413,
     "status": "ok",
     "timestamp": 1679821639887,
     "user": {
      "displayName": "bel hello",
      "userId": "14145485794765337692"
     },
     "user_tz": -480
    },
    "id": "2lm2HNfGVwuF",
    "outputId": "4a9006e2-3f97-488f-88b2-010bebe1fbf7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41443,
     "status": "ok",
     "timestamp": 1679821681325,
     "user": {
      "displayName": "bel hello",
      "userId": "14145485794765337692"
     },
     "user_tz": -480
    },
    "id": "-Sd_3u819e-V",
    "outputId": "2017eb54-347e-40c1-e0bc-b0e81046c782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting deepmultilingualpunctuation\n",
      "  Downloading deepmultilingualpunctuation-1.0.1-py3-none-any.whl (5.4 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from deepmultilingualpunctuation) (4.27.3)\n",
      "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from deepmultilingualpunctuation) (1.13.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (4.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers->deepmultilingualpunctuation) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers->deepmultilingualpunctuation) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->deepmultilingualpunctuation) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers->deepmultilingualpunctuation) (0.13.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->deepmultilingualpunctuation) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->deepmultilingualpunctuation) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->deepmultilingualpunctuation) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers->deepmultilingualpunctuation) (1.22.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->deepmultilingualpunctuation) (3.10.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->deepmultilingualpunctuation) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->deepmultilingualpunctuation) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->deepmultilingualpunctuation) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->deepmultilingualpunctuation) (1.26.15)\n",
      "Installing collected packages: deepmultilingualpunctuation\n",
      "Successfully installed deepmultilingualpunctuation-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install deepmultilingualpunctuation\n",
    "from deepmultilingualpunctuation import PunctuationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17870,
     "status": "ok",
     "timestamp": 1679821699182,
     "user": {
      "displayName": "bel hello",
      "userId": "14145485794765337692"
     },
     "user_tz": -480
    },
    "id": "0DCl8Y-62n0j",
    "outputId": "33f8149f-c840-40ef-ed5f-339c533b4ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ruptures\n",
      "  Downloading ruptures-1.1.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from ruptures) (1.10.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from ruptures) (1.22.4)\n",
      "Installing collected packages: ruptures\n",
      "Successfully installed ruptures-1.1.7\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.27.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.13.1+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.14.1+cu116)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.1.97)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=21f6c528cedd06ecead94eaca7ae9179a938dce6deff6217a21049fd7172a3c0\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-2.2.2\n"
     ]
    }
   ],
   "source": [
    "#For segmentation of transcript into product reviews\n",
    "!pip install ruptures\n",
    "!pip install sentence_transformers\n",
    "import ruptures as rpt\n",
    "from ruptures.base import BaseCost\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntIFHR7mUQhu"
   },
   "source": [
    "Intialise Classes for Product Review Identification and Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 619,
     "status": "ok",
     "timestamp": 1679821699774,
     "user": {
      "displayName": "bel hello",
      "userId": "14145485794765337692"
     },
     "user_tz": -480
    },
    "id": "W80RtDhVkcwE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ProductIdentifier:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def remove_non_ascii(self, text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "\n",
    "  def GetNounsWithNLTK(self, text, chunk_func=ne_chunk):\n",
    "    chunked = chunk_func(pos_tag(word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for subtree in chunked:\n",
    "      if type(subtree) == Tree:\n",
    "        current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "      elif current_chunk:\n",
    "        named_entity = \" \".join(current_chunk)\n",
    "        if named_entity not in continuous_chunk:\n",
    "          continuous_chunk.append(named_entity)\n",
    "          current_chunk = []\n",
    "      else:\n",
    "        continue\n",
    "\n",
    "    return continuous_chunk\n",
    "\n",
    "  def get_uppercased_word_series(self, text):\n",
    "    temp_lst = []\n",
    "    uppercased_word_lst = []\n",
    "    word_lst = nltk.word_tokenize(text)\n",
    "    for i in range(len(word_lst)):\n",
    "      word = word_lst[i]\n",
    "      starts_with_uppercase = word[0].isupper()\n",
    "      if starts_with_uppercase:\n",
    "        temp_lst.append(word)\n",
    "        if (i == len(word_lst)-1):\n",
    "          if len(temp_lst) >= 2:\n",
    "            uppercased_word_lst.append(\" \".join(temp_lst))\n",
    "        continue\n",
    "      else:\n",
    "        if len(temp_lst) >= 2:\n",
    "          uppercased_word_lst.append(\" \".join(temp_lst))\n",
    "        temp_lst = []\n",
    "    return list(set(uppercased_word_lst))\n",
    "\n",
    "  def Check_If_NP_Is_Known_Product(self,np, product_lexicon):\n",
    "    for product in product_lexicon:\n",
    "      if np == product:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "  def Check_If_NP_Has_Product_Indicator(self, np, product_indicators):\n",
    "    for product in product_indicators:\n",
    "      similarity = fuzz.partial_ratio(np.lower(),product.lower())\n",
    "      if similarity >= 80:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "  def Check_If_NP_Is_Product(self, np, product_indicators, product_lexicon):\n",
    "    if self.Check_If_NP_Is_Known_Product(np, product_lexicon):\n",
    "      return True\n",
    "    elif self.Check_If_NP_Has_Product_Indicator(np, product_indicators):\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "  def is_not_substring_of_another_product(self,product, product_lst):\n",
    "    for comparison_product in product_lst:\n",
    "      if product.lower() == comparison_product.lower():\n",
    "        continue\n",
    "      if product.lower() in comparison_product.lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "  def detect_products_from_transcript(self, text, product_indicators, product_lexicon):\n",
    "    #sometimes nltk ne chunker misses out\n",
    "    nouns_in_transcript = self.GetNounsWithNLTK(text)\n",
    "    #parse for a series of uppercase\n",
    "    uppercased_series_lst = self.get_uppercased_word_series(text)\n",
    "\n",
    "    #nouns_in_transcript.extend(np_lst)\n",
    "    nouns_in_transcript.extend(uppercased_series_lst)\n",
    "\n",
    "    #only keep if len > 2 \n",
    "    nouns_in_transcript = list(filter(lambda noun_phrase: len(nltk.word_tokenize(noun_phrase))>=2, nouns_in_transcript))\n",
    "\n",
    "    #keep if match with brand name or contain a product term\n",
    "    nouns_in_transcript = list(filter(lambda noun_phrase: self.Check_If_NP_Is_Product(noun_phrase, product_indicators, product_lexicon), nouns_in_transcript))\n",
    "\n",
    "    #filter the products\n",
    "    nouns_in_transcript = list(filter(lambda product: self.is_not_substring_of_another_product(product, nouns_in_transcript), nouns_in_transcript))\n",
    "\n",
    "    return list(set(nouns_in_transcript))\n",
    "\n",
    "\n",
    "  def contains_url(self,text):\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.findall(regex, text)\n",
    "    if url != []:\n",
    "      return True\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "  def remove_url(self,text):\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    removed_url_text = re.sub(regex, \"\", text)\n",
    "    return removed_url_text\n",
    "\n",
    "  def clean_desc(self,text):\n",
    "    text = text.replace(\":\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = self.remove_non_ascii(text)\n",
    "    text = self.remove_url(text)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    text = text.replace(\"https\",\"\")\n",
    "    return text.strip()\n",
    "\n",
    "  \n",
    "  def GetNPWithSyntaxRules(self, text):\n",
    "    nounphrases = []\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    chunk_name_lst = [\"NP\",\"Product\"]\n",
    "    grammar = r\"\"\"  NP:\n",
    "              {<NN|NNS|NNP|NNPS>+}\n",
    "              Product:\n",
    "              {<NN.*|NNS.*|NNP.*|NNPS.*><NN|VBZ|JJ|JJS|JJR|NNS|NNP|NNPS|VBD|CD|RB>+}\n",
    "          \"\"\"\n",
    "    chunkParser=nltk.RegexpParser(grammar)\n",
    "    tree = chunkParser.parse(tagged)\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() in chunk_name_lst):\n",
    "        myPhrase = ''\n",
    "        for item in subtree.leaves():\n",
    "            myPhrase += ' ' + item[0]\n",
    "        nounphrases.append(myPhrase.strip())\n",
    "    nounphrases = list(filter(lambda x: len(x.split()) > 1, nounphrases))\n",
    "    return nounphrases\n",
    "\n",
    "  def get_products_from_desc(self,description):\n",
    "    if description == None:\n",
    "      return []\n",
    "    split_desc = description.split(\"\\n\")\n",
    "    #set higher threshold because even if e.g. \"mal\" \"ibu\" match seperately -- it raises the match score\n",
    "    products = []\n",
    "    for sent in split_desc:\n",
    "      if sent != \"\":\n",
    "        products.extend(self.GetNPWithSyntaxRules(sent))\n",
    "    products = [self.clean_desc(txt) for txt in products]\n",
    "    products = list(filter(lambda txt: self.Check_If_NP_Is_Product(txt, product_indicators, product_lexicon),products))\n",
    "    products = list(filter(lambda txt: self.is_not_substring_of_another_product(txt,products),products))\n",
    "    return products\n",
    "  \n",
    "  def detect_products(self,text,description,product_indicators, product_lexicon):\n",
    "    #filtered_transcript_product_lst = self.detect_products_from_transcript(text, product_indicators, product_lexicon)\n",
    "    filtered_transcript_product_lst = self.get_products_from_desc(description)\n",
    "    #filtered_transcript_product_lst.extend(filtered_desc_product_lst)\n",
    "    filtered_product_lst = list(filter(lambda txt: self.is_not_substring_of_another_product(txt,filtered_transcript_product_lst),filtered_transcript_product_lst))\n",
    "    filtered_product_lst = list(set(filtered_product_lst))\n",
    "    return filtered_product_lst\n",
    "\n",
    "class CoreferenceResolver:\n",
    "  def __init__(self):\n",
    "    self.product_identifier = ProductIdentifier()\n",
    "  \n",
    "  def GetCoreferenceWithSyntaxRules(self,text):\n",
    "    coref = []\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    chunk_name_lst = [\"DT\",\"DT Ref\"]\n",
    "    grammar = r\"\"\"\n",
    "              DT Ref:\n",
    "              {<DT><CD>}\n",
    "          \"\"\"\n",
    "    chunkParser=nltk.RegexpParser(grammar)\n",
    "    tree = chunkParser.parse(tagged)\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() in chunk_name_lst):\n",
    "      myPhrase = ''\n",
    "      for item in subtree.leaves():\n",
    "        myPhrase += ' ' + item[0]\n",
    "      coref.append(myPhrase.strip())\n",
    "    coref = list(filter(lambda x: len(x.split()) > 1, coref))\n",
    "    filtered_coref = list(set(coref))\n",
    "    return filtered_coref\n",
    "\n",
    "  def replace_all(self, text, dic):\n",
    "    for i, j in dic.items():\n",
    "        text = re.sub(r\"\\b%s\\b\" % i, j, text)\n",
    "        # r\"\\b%s\\b\"% enables replacing by whole word matches only\n",
    "    return text\n",
    "\n",
    "  def detect_matching_product_name(self, sentence, product_lst,product_indicators, product_lexicon):\n",
    "    np_lst = self.product_identifier.GetNPWithSyntaxRules(sentence)\n",
    "    for np in np_lst:\n",
    "      for product in product_lst:\n",
    "        similarity_with_identified_products = fuzz.partial_ratio(np.lower(),product.lower())\n",
    "        if similarity_with_identified_products >= 80 or np in product:\n",
    "          return (np,product)\n",
    "    for np in np_lst:\n",
    "      if self.product_identifier.Check_If_NP_Is_Product(np, product_indicators, product_lexicon):\n",
    "        return (np,np)\n",
    "    return None\n",
    "\n",
    "\n",
    "  def resolve(self, sample_sentences, product_lst, product_indicators, product_lexicon):\n",
    "    curr_product_name = None\n",
    "    resolved_combined_sentence_list = []\n",
    "    for sentence in sample_sentences:\n",
    "      #update current product name\n",
    "      potentially_new_product_name_pair = self.detect_matching_product_name(sentence, product_lst,product_indicators, product_lexicon)\n",
    "      if potentially_new_product_name_pair != None and potentially_new_product_name_pair != curr_product_name:\n",
    "        #must identify the product name better\n",
    "        phrase = potentially_new_product_name_pair[0]\n",
    "        curr_product_name = potentially_new_product_name_pair[1]\n",
    "        #pos_tag sentence --> assign determiners after the product name to the product name\n",
    "        substr_before_product = sentence.split(phrase,1)[0] + curr_product_name\n",
    "        substr_after_product = sentence.split(phrase,1)[-1]\n",
    "        replacements = {'This':curr_product_name,'this':curr_product_name,'it':curr_product_name, 'It':curr_product_name}\n",
    "        substr_after_product = self.replace_all(substr_after_product, replacements)\n",
    "        resolved_combined_sentence_list.append(substr_before_product + substr_after_product)\n",
    "      \n",
    "      elif curr_product_name != None:\n",
    "        #replace all references with the curr_product_name\n",
    "        replacements = {'This':curr_product_name,'this':curr_product_name,'it':curr_product_name, 'It':curr_product_name}\n",
    "        sentence = self.replace_all(sentence, replacements)\n",
    "        resolved_combined_sentence_list.append(sentence)\n",
    "      \n",
    "      else:\n",
    "        resolved_combined_sentence_list.append(sentence)\n",
    "    \n",
    "    return resolved_combined_sentence_list\n",
    "\n",
    "  def resolve_sentences(self,sample_sentences, product_lst,product_indicators, product_lexicon):\n",
    "    sample_sentences = [sentence.replace(\"\\n\", \" \") for sentence in sample_sentences]\n",
    "    resolved_list = self.resolve(sample_sentences, product_lst,product_indicators, product_lexicon)\n",
    "    return resolved_list\n",
    "\n",
    "#Define cost function for segmentation\n",
    "class CosineCost(BaseCost):\n",
    "    \"\"\"Cost derived from the cosine similarity.\"\"\"\n",
    "\n",
    "    # The 2 following attributes must be specified for compatibility.\n",
    "    model = \"custom_cosine\"\n",
    "    min_size = 2\n",
    "\n",
    "    def fit(self, signal):\n",
    "        \"\"\"Set the internal parameter.\"\"\"\n",
    "        self.signal = signal\n",
    "        self.gram = util.cos_sim(signal, signal)\n",
    "        return self\n",
    "\n",
    "    def error(self, start, end) -> float:\n",
    "        \"\"\"Return the approximation cost on the segment [start:end].\n",
    "\n",
    "        Args:\n",
    "            start (int): start of the segment\n",
    "            end (int): end of the segment\n",
    "        Returns:\n",
    "            segment cost\n",
    "        Raises:\n",
    "            NotEnoughPoints: when the segment is too short (less than `min_size` samples).\n",
    "        \"\"\"\n",
    "        if end - start < self.min_size:\n",
    "            raise NotEnoughPoints\n",
    "        sub_gram = self.gram[start:end, start:end]\n",
    "        val = sub_gram.diagonal().sum()\n",
    "        val -= sub_gram.sum() / (end - start)\n",
    "        return val\n",
    "\n",
    "class TranscriptSegmenter:\n",
    "  def __init__(self):\n",
    "    self.product_identifier = ProductIdentifier()\n",
    "    self.coreference_resolver = CoreferenceResolver()\n",
    "    self.punctuator = PunctuationModel()\n",
    "    self.segmentation_cost_function_class = CosineCost()\n",
    "    self.sentence_embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "  def further_split_sentence(self, nltk_sent_tokenized_list):\n",
    "    #use punctuator to improve splitting\n",
    "    final_sent_lst = []\n",
    "    for sent in nltk_sent_tokenized_list:\n",
    "      punctuated_sent = self.punctuator.restore_punctuation(sent)\n",
    "      split_sent = punctuated_sent.split(\".\")\n",
    "      final_sent_lst.extend(split_sent)\n",
    "    return final_sent_lst\n",
    "\n",
    "  def get_sentence_list_from_transcript(self, transcript):\n",
    "    transcript = self.product_identifier.remove_non_ascii(transcript)\n",
    "    phrases = self.further_split_sentence(nltk.sent_tokenize(transcript))\n",
    "    if len(phrases) == 0:\n",
    "      return []\n",
    "    phrases = list(filter(lambda phrase: phrase != \"\", phrases))\n",
    "    pos_tagged = [pos_tag(nltk.word_tokenize(sentence.lower())) for sentence in phrases]\n",
    "    new_combined_sentence_list = [\"\"]\n",
    "    for i in range(len(phrases)):\n",
    "      tagged_phrase = pos_tagged[i]\n",
    "      tag_of_first_word = tagged_phrase[0][1]\n",
    "      conjunctions_lst = [\"IN\", \"CC\", \"WDT\", \"TO\", \"WRB\", \"VBG\", \"VBZ\", \"RB\"]\n",
    "      end_lst = [\"IN\", \"CC\", \"WDT\", \"TO\", \"WRB\", \"VBG\", \"VBZ\", \"DT\", \"RB\"]\n",
    "      join_sentence = False\n",
    "      if i >= 1:\n",
    "        last_tag_in_prev_sentence = pos_tagged[i-1][-1][1]\n",
    "        if last_tag_in_prev_sentence in end_lst:\n",
    "          join_sentence = True\n",
    "      if tag_of_first_word in conjunctions_lst:\n",
    "        join_sentence = True\n",
    "\n",
    "      if join_sentence:\n",
    "        #lowercase the first letter to reduce noise when doing NP identification e.g. \"Which is so beautiful\" --> Which to which\n",
    "        sub_clause = phrases[i].replace(phrases[i][0],phrases[i][0].lower(),1)\n",
    "        new_combined_sentence_list[-1] = new_combined_sentence_list[-1] + \" \" + sub_clause\n",
    "      else:\n",
    "        new_combined_sentence_list.append(phrases[i])\n",
    "    return new_combined_sentence_list\n",
    "\n",
    "  def segment_transcript_ruptures(self, filtered_product_lst, resolved_sentences, original_sentences, product_indicators, product_lexicon):\n",
    "    #Find breaks where similarity between chunks is the lowest\n",
    "    num_products = len(filtered_product_lst)\n",
    "    embeddings = self.sentence_embedding_model.encode(resolved_sentences, convert_to_tensor=True)\n",
    "    n_bkps = num_products-1+2  # e.g. there are 9 change points (10 text segments) + intro and outrp\n",
    "    min_size = 2\n",
    "    jump = 1\n",
    "    #to create valid segmentation paramters\n",
    "    if n_bkps > len(embeddings) or n_bkps * math.ceil(min_size/ jump) * jump + min_size > len(embeddings): \n",
    "      n_bkps = round(math.ceil(min_size/ jump) * jump + min_size)\n",
    "    algo = rpt.Dynp(custom_cost=self.segmentation_cost_function_class, min_size=min_size, jump=jump).fit(embeddings)\n",
    "    predicted_bkps = algo.predict(n_bkps=n_bkps)\n",
    "    iteration_lst = predicted_bkps\n",
    "    iteration_lst.insert(0, 0)\n",
    "\n",
    "    chunk_lst = []\n",
    "    product_review_dict = {}\n",
    "    for i in range(len(iteration_lst)-1):\n",
    "      chunk_resolved = \" \".join(resolved_sentences[iteration_lst[i]:iteration_lst[i+1]])\n",
    "      chunk_org = \" \".join(original_sentences[iteration_lst[i]:iteration_lst[i+1]])\n",
    "      product = self.coreference_resolver.detect_matching_product_name(chunk_resolved, filtered_product_lst,product_indicators, product_lexicon)\n",
    "      if product != None:\n",
    "        product_review_dict[product[1]] = {\"original\":chunk_org}\n",
    "    return product_review_dict\n",
    "\n",
    "  def segment_transcript(self, filtered_product_lst, resolved_list, new_combined_sentence_list):\n",
    "    product_review_dict = {}\n",
    "    for i in range(len(resolved_list)):\n",
    "      resolved_sentence = resolved_list[i]\n",
    "      for product in filtered_product_lst:\n",
    "        if product in resolved_sentence:\n",
    "          if product in product_review_dict:\n",
    "            #product_review_dict[product][\"resolved\"] = product_review_dict[product][\"resolved\"] + resolved_sentence\n",
    "            product_review_dict[product][\"original\"] = product_review_dict[product][\"original\"] + new_combined_sentence_list[i]\n",
    "          else:\n",
    "            product_review_dict[product] = {\"original\":new_combined_sentence_list[i]}\n",
    "    \n",
    "    return product_review_dict\n",
    "\n",
    "  def split_transcript_by_detected_products(self, review_dict, product_indicators, product_lexicon):\n",
    "    transcript = review_dict[\"transcript_text\"]\n",
    "    description = review_dict[\"description\"]\n",
    "    filtered_product_lst = self.product_identifier.detect_products(transcript, description, product_indicators, product_lexicon)\n",
    "    sample_sentences = self.get_sentence_list_from_transcript(transcript)\n",
    "    resolved_lst = self.coreference_resolver.resolve_sentences(sample_sentences, filtered_product_lst,product_indicators, product_lexicon)\n",
    "    #product_review_dict = self.segment_transcript(filtered_product_lst, resolved_lst, sample_sentences)\n",
    "    product_review_dict = self.segment_transcript_ruptures(filtered_product_lst, resolved_lst, sample_sentences, product_indicators, product_lexicon)\n",
    "    return product_review_dict\n",
    "\n",
    "  #Case of video having chapters\n",
    "\n",
    "  def process_raw_transcript(self, raw_transcript):\n",
    "    text_list = [segment[\"text\"] for segment in raw_transcript]\n",
    "    text = \" \".join(text_list)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    #remove non-ascii characters -- to remove foreign lang characters\n",
    "    text = self.product_identifier.remove_non_ascii(text)\n",
    "    return text\n",
    "\n",
    "  def split_transcript_by_chapters(self, raw_transcript, chapters):\n",
    "    starts = [round(segment[\"start\"]) for segment in raw_transcript]\n",
    "    product_review_dict = {}\n",
    "    chapter_index_list = []\n",
    "    for chapter in chapters:\n",
    "      time_in_sec = round(chapter[\"time_in_min\"] * 60)\n",
    "      #get a index list of chapters \n",
    "      new_insertion_index = bisect.bisect_left(starts, time_in_sec)\n",
    "      chapter_index_list.append(new_insertion_index)\n",
    "    chapter_index_list.append(len(raw_transcript)-1)\n",
    "    for i in range(len(chapter_index_list)-1):\n",
    "      title = chapters[i][\"title\"]\n",
    "      start_index = chapter_index_list[i]\n",
    "      end_index = chapter_index_list[i+1]\n",
    "      original_raw_transcript = self.process_raw_transcript(raw_transcript[start_index:end_index])\n",
    "      sample_sentences = [segment[\"text\"] for segment in raw_transcript[start_index:end_index]]\n",
    "      #resolved_raw_transcript = self.coreference_resolver.resolve_sentences(sample_sentences, [title])\n",
    "      product_review_dict[title] = {\"original\":original_raw_transcript}\n",
    "    return product_review_dict\n",
    "\n",
    "\n",
    "  def get_product_review_dict(self, review_dict, product_indicator, product_lexicon):\n",
    "    chapters = sample_review_dict[\"chapters\"]\n",
    "    product_review_dict = {}\n",
    "    if chapters == []:\n",
    "      product_review_dict = self.split_transcript_by_detected_products(review_dict, product_indicator, product_lexicon)\n",
    "    else:\n",
    "      raw_transcript = sample_review_dict[\"transcript_raw\"]\n",
    "      product_review_dict = self.split_transcript_by_chapters(raw_transcript, chapters)\n",
    "    return product_review_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWQZZLavdKXK"
   },
   "source": [
    "Load Youtube Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1679821831422,
     "user": {
      "displayName": "bel hello",
      "userId": "14145485794765337692"
     },
     "user_tz": -480
    },
    "id": "6T73mlgljGzM"
   },
   "outputs": [],
   "source": [
    "import bisect\n",
    "import json\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDcmoMkdcQgc"
   },
   "outputs": [],
   "source": [
    "def get_beauty_lexicon(path):\n",
    "    beauty_lexicon = []\n",
    "    df = pd.read_csv(path)\n",
    "    beauty_lexicon = df[\"Lexicon\"].unique().tolist()\n",
    "    return beauty_lexicon\n",
    "\n",
    "product_indicators = get_beauty_lexicon(\"beauty_lexicon.csv\")\n",
    "product_lexicon = pd.read_csv(\"Product Lexicon.csv\")[\"Product\"].to_list()\n",
    "\n",
    "#will take longer on initial load due to loading of punctuator model\n",
    "transcript_segmenter = TranscriptSegmenter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFyuTXRVKn3a"
   },
   "source": [
    "Demo Example (On video without chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5183,
     "status": "ok",
     "timestamp": 1679822015315,
     "user": {
      "displayName": "bel hello",
      "userId": "14145485794765337692"
     },
     "user_tz": -480
    },
    "id": "xWIhQ26XKpKN",
    "outputId": "083debe0-1745-498b-e8e9-407d097e385b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Without chapters\n",
      "makeup looks\n",
      "{'original': 'Thank you guys, so much for watching let me know what other makeup looks you want me to do Bye'}\n",
      "Becca Undereye Brightening Corrector\n",
      "{'original': 'I m going in with the Becca Under  Eye Brightening Corrector and even though it s for eyes, I use it everywhere I need to correct discoloration then I m using the Too Faced Born This Way Concealer This stuff is very full coverage I wait 10 seconds before blending to give it time to set and we run it back And you guys know I have to go in twice with it to really cover up those dark circles for foundation  I m just applying dots to cover my face and I make sure to blend it out really well and, yes, always get that hairline and neck'}\n",
      "Laura Mercier Setting Powder\n",
      "{'original': 'I m setting my face with the Laura Mercier Setting Powder I go in with a sponge, first to set under my eyes, and then with a brush to lightly dust the rest of my face now I m using the rem'}\n",
      "beauty Midnight Shadows Eyeshadow Palette\n",
      "{'original': 'beauty, Midnight Shadows, Eyeshadow Palette and using the shade Eggnog to lightly go over my whole eye I m going to take Sashay Sorbet to slowly build up the color in the outer corner and then the shade Plumbledore blending up and over where my crease would be then, using the shade Cheeky Hijiki, I m going in to the same area and making it darker to give a smokier effect with an angled brush  I m taking that same color to my lower eyelid and connecting it with with my outer corner Next, I m going with the shade whipped cream and putting it right in the center and inner corner of my eye My palette is literally broken  To give my inner corners and lower eyelid a bit more pop, I m taking Play Hard by Colour Pop and focusing it here and blending it out with a brush for eyeliner'}\n",
      "smudger brush\n",
      "{'original': ' I m using a smudger brush and taking Cheeky Hijiki again, drawing my starting line at the outer corner and then connecting it with the rest of my eye I m also taking it on my lower lid and cleaning it up with a Q-tip for lashes'}\n",
      "stila Stay All Day liner\n",
      "{'original': ' you guys know that my go-to are these ones from Amazon then I m taking the stila Stay All Day liner and lining my inner corners and connecting it with the lash I also use this to fill in the gap between by waterline and the lash band'}\n",
      "CLIO Kill Lash Superproof Mascara\n",
      "{'original': 'I love using this CLIO Kill Lash Mascara to do my bottom lashes, since the wand is really skinny and can get every little hair for brows, I m taking a brow mascara and just giving the hairs more body then, using the Benefit Goof Proof Brow Pencil, I m filling the rest in making sure to comb them throughly i m going to do a little bit of nose contour, just going over the hollow space between my brows and the bridge of my nose You can tell I love this blush from bareMinerals and I apply it to my cheekbones using upward sweeping motions for highlighter  I m using this eyeshadow from Colour Pop and putting it where I put my blush'}\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "With chapters\n",
      "Selena Gomez Pore Diffusing Primer\n",
      "{'original': \" it does say that it's a poor diffusing primer, so hopefully that's case, because I do have quite a bit of pores  but my initial impression is that it feels so good and it definitely does feel like a lightweight version of those kind of silicone primers of that kind of benefit porefessional  it feels something like that type of texture, but super light in comparison  it almost feels gel like on my skin  so it feels so good  I feel it drawing down, so let's see what happens when it dries down, because it does feel like it's kind of turning a little bit mattified, maybe even I'm not sure\"}\n",
      "precision brush feels\n",
      "{'original': ' oh, it feels really nice  it does seem to be  it seems to be a hybrid between a serum and kind of like one of those poor filling primers, so actually it feels like a very light pore filling primer'}\n",
      "makeup makeup look\n",
      "{'original': \" I feel like my skin looks so much smoother  it definitely feels super soft to the touch and the pores definitely look reduced  but let's put the makeup on top, because that's when I really see a huge difference  when I apply the makeup on top of the primer- but already I can- I can definitely see a difference  so now the next product that I have to try here is the positive light under eye brightener  now they said that you can wear this under your makeup for a no makeup makeup look, or you can apply this on top of your concealer for that ultimate brightening effect  I'm going to try it both ways and let's see what happens  so here it is: beautiful packaging  okay, I had heard about this  they said there's, you know, the rollerball at the end and some some people were saying they like it and some people don't  but let me go ahead and give it a try  it definitely is cold, so it's cooling and will definitely have that puffing benefit  so it definitely does feel really good and does seem to be working, even as a concealer  because I have a lot of under eye brighteners and they're not always this opaque  sometimes they're more sheer  generally, the ones that I've tried are more sheer, so this one does seem to have more coverage than the other under eye brighteners that I have, so I'm just going to go ahead and tap it in  let me just go ahead and use my finger  oh, wow, so it definitely melted into my under eye area pretty much immediately  it's not dry, so it's not tugging on my under eye  it's very creamy and smooth, so it actually does almost even feel like an eye cream  it's beautiful, wow, awesome  expecting that you guys  seriously, this is a lot of coverage  usually I do color correct, so this actually color correct\"}\n",
      "color correctors\n",
      "{'original': \" I don't feel like I need the color correctors  what I'm trying to say? wow, it's very soothing, so it's not like a concealer and so many concealers just tug your under eye  they're kind of dry in texture  this feels like an eye cream and it just concealed like a concealer  how amazing is that? yeah, I could definitely see going out like this and feeling you know, like you don't need any other makeup\"}\n",
      "makeup makeup product s\n",
      "{'original': \" this is that  that's definitely why they said it's a no makeup makeup product s  so should I eat? I don't even want to put on any more makeup  guys, this feels so good  it's crazy\"}\n",
      "Beauty Tinted Moisturizer\n",
      "{'original': \" I do have to hear my rare Beauty Foundation, though actually this looks kind of deep for me  now I have the Tinted Moisturizer  uh, what do you guys think this is the positively attention should I put? maybe I'll put a little  okay, let's put a little bit of this on, just so I can see how it sits with the primer  so this is just the tinted moisturizer  it's pretty  this doesn't have SPF 20 in it and very lightweight  does give you a glow  I've been wearing this for a while- and I love it  this primer definitely did a lot for my skin, because normally I don't feel comfortable with just a tinted moisturizer on, but because the primer did so much to disguise the pores and smooth my skin that the Tinted Moisturizer is just a perfect match if you don't feel like you need more coverage or you know trying  a lot of times I use Foundation to kind of smooth my skin and disguise the pores, but this primer did it already without a foundation  so that's really amazing  so I had been seeing a lot of people online use their rare Beauty blush and they put it on before the concealer  so I do have my concealer here, also from rare Beauty\"}\n",
      "Beauty liquid blush Beauty liquid blush\n",
      "{'original': \" it's almost out, but I'm going to try that  have I done that before? I'm not sure, but I'm going to go ahead and take my rare Beauty liquid blush  this is happy  this product is so pigmented  you only need the tiniest amount  that probably might even have been too much, right there, you really just need a little bit, but let me go ahead and just press that into my skin  yeah, I think I- I kind of I did put too much, but the thing about it is that you can put the concealer on top and it takes down the blush and then what winds up happening is the blush peeks out from under the concealer and it just makes the most gorgeous glow  yes, I'll just take the concealer now and let's go ahead and put this right under my eyes  and this is some kind of irritation that I got as well from this product that I tried  so this is at this circle  right here is not black, it looks like  the blush is just like won't blend  now it's it's um, it's like a, it looks like a burn, but you know, it's from the something I tried, I don't know why  so loving that  and now I'm just going to go in with a little bit more of the brightener just to try it on top of the concealer  so I'm just going to put it right on top  so I'm just going to go ahead and set the under eye area and that concealer  this is why it doesn't crease up  so, yeah, my skin looks so smooth  it's definitely from that primer  so that primary is bondcom  so now I'm going to apply the highlighter\"}\n",
      "Highlighter Brush\n",
      "{'original': \" this is the enlightened highlighter and this is the highlighter brush  so I'm just gonna go ahead and apply this  oh, wow, that's beautiful  oh, my God, I don't know if I've ever had a highlighter like this before  that looks wet, it's so glossy, it's just stunning and it feels so silky on your skin  so I'm so used to highlighters being chunky or accentuating my texture, but this is a dream  seriously, it's unbelievable  I've been trying to get this look for the longest time and normally to get this look, I'll put on a highlighter and then I'll put kind of a balm on top one of those transparent bombs, because I love that wet look  but look at that  that just did it in one second  it just gave the wet look  oh, my gosh, in one second  keep putting it up  but it feels so good  how many highlighters are just chunky on skin and just drying and just I can't wear them  you know, a lot of times I think it's in my skin, too much texture  but you know, I don't know  but this is what I could wear  this is beautiful, gives me the look that I wanted, wow  and then there's this shade  this one is called um flaunt  and then there's mesmerized  let's see what that one looks like  oh, that is just so beautiful  this is mesmerized  it has pink Shimmer to it  wow, this one's the one that I just showed you for want, has a golden Shimmer to it  and then let's see this one here  I'll Swatch them out as well  um, this one is called an auxiliary  oh my gosh, guys  I just my nail just hit it when I was opening it  okay, that's all right, though I'm still gonna wear it  I'm just gonna spray my face with my normal routine  I just realize these aren't even wet  so how would that look now that my skin or wet my brush? you know, let's just see what happens if you wet the brush just for lunch  I don't think you even need it, but let's just, you know, play and have fun  so this is that first shade enlightened  and this is it wet  yeah, it's wow  it's even more gorgeous  is that possible? that's crazy celebrities\"}\n",
      "Rare Beauty Kind Words Lip Liner\n",
      "{'original': \" I just went ahead and added some lashes, some liner and some bronzer, but I have truly never seen such a gorgeous highlighter in my life  I'm not joking  I wish ugh  I wish you could all try at least  if you could get in store and try it for yourself, you will not believe it  um, of course they do have it at Sephora  so hopefully you can get into a Sephora and experience this for yourself, because it's flipping amazing  and the primer, seriously, that new primer is phenomenal  so I don't have foundation on right now  I literally can't believe it  I always need my Foundation  it's kind of like a security blanket for me, but uh, I don't have it on right now and I feel like my skin looks so good  I definitely put it down to that primer  so that primer is truly amazing  the eye brightener is beautiful as well  I forgot to say I did put some of the highlighter also on my eyes because it was so stunning, so I just put it on the front portion of my eye  I also did add a little bit of a eyeshadow too, so I forgot about that  but when I was doing my eyes, I used this as kind of on my lid and in the tear duct area and it was so I thought it was just so stunning  so it definitely made my eyes look more wide awake and more well rested and all kinds of good stuff  so you just take a small brush and just put it  you know, like is it the word eyeshadow? so thank you very, so much for coming and spending time with me today, and that's pretty much it, so I hope you guys are all doing good and I'll see you very, very soon\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#demo on yjYaxtgVfyg from Feb23 data\n",
    "input_json_filename = \"Feb23.json\"\n",
    "output_json_filename = \"PRI_Output_\"+input_json_filename\n",
    "#Load data scraped using various Youtube API\n",
    "with open(input_json_filename, 'r') as f:\n",
    "  reviews = json.load(f)\n",
    "\n",
    "sample_review_dict = reviews[\"yjYaxtgVfyg\"]\n",
    "product_review_dict = transcript_segmenter.get_product_review_dict(sample_review_dict, product_indicators, product_lexicon)\n",
    "product_review_dict\n",
    "\n",
    "print(\"Without chapters\")\n",
    "for k, v in product_review_dict.items():\n",
    "  print(k)\n",
    "  print(v)\n",
    "\n",
    "sample_review_dict_with_chapters = reviews['QZuisQ0Z6Sk']\n",
    "product_review_dict_chapters = transcript_segmenter.get_product_review_dict(sample_review_dict_with_chapters, product_indicators, product_lexicon)\n",
    "\n",
    "print(\"With chapters\")\n",
    "for k, v in product_review_dict_chapters.items():\n",
    "  print(k)\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQdJRYEpat5e"
   },
   "source": [
    "Loop to get all review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdlLE_u4gGbb"
   },
   "outputs": [],
   "source": [
    "input_json_filename = \"Feb23.json\"\n",
    "output_json_filename = \"PRI_Output_\"+input_json_filename\n",
    "#Load data scraped using various Youtube API\n",
    "with open(input_json_filename, 'r') as f:\n",
    "  reviews = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gA-KlhbvrLu"
   },
   "outputs": [],
   "source": [
    "review_id_lst = []\n",
    "for review_id, review_data in reviews.items():\n",
    "  review_id_lst.append(review_id)\n",
    "print(len(review_id_lst))\n",
    "print(review_id_lst)\n",
    "\n",
    "compiled_transcript_dict = {}\n",
    "for review_id in review_id_lst:\n",
    "  sample_review_dict = reviews[review_id]\n",
    "  product_review_dict = transcript_segmenter.get_product_review_dict(sample_review_dict, product_indicators, product_lexicon)\n",
    "  compiled_transcript_dict[review_id] = product_review_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axuuroAT6oCo"
   },
   "outputs": [],
   "source": [
    "with open(output_json_filename, \"w\") as write_file:\n",
    "   json.dump(compiled_transcript_dict, write_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1GLvlKvhzw0R1c70AnlQ5INKXs2b6lbJL",
     "timestamp": 1678625474195
    },
    {
     "file_id": "1n3UKjT3OQQhgSSzgPXVV41cPLm1VcYci",
     "timestamp": 1678521039732
    },
    {
     "file_id": "1vM36DwbvXyWlAIjmbVfevvtOh-ZNUeuj",
     "timestamp": 1678340160592
    },
    {
     "file_id": "1NnueS5Mm9Ashjv8CZ8srV-Po8Wi-4Tc3",
     "timestamp": 1677679513975
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
